{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sparse_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1RCg-G5JoDj-dfC-Lx_PhiBm6IDTJij1y",
      "authorship_tag": "ABX9TyOo3OuUw/tfJRPv5zP21Gfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niharikajo/Sparse-Attention/blob/main/sparse_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting Drive**\n"
      ],
      "metadata": {
        "id": "JU3Iy4-naF7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k5zlRmbXanxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "change directory path to Sparse-Attention-main folder"
      ],
      "metadata": {
        "id": "-XMT2W3iaLkn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77TKFjKU1Zv8",
        "outputId": "410b87b4-0128-4774-e2b7-f726d3fe9223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Sparse-Attention-main\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Sparse-Attention-main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYMV76kdaOvt",
        "outputId": "d00ae57e-e6db-4693-c55f-8eca5c5d218f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.py\tipc.py\t    __pycache__       results\t train.py\n",
            "data\t\tmetrics.py  README.md\t      run-ds.py  utils\n",
            "data_loader.py\tmodel.py    requirements.txt  settings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install missing packages"
      ],
      "metadata": {
        "id": "zjCNW-6jadEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deepspeed"
      ],
      "metadata": {
        "id": "SQaPCqGv2V8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39206ae6-f136-4213-ab72-ed7cb28ac893"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.6.4.tar.gz (556 kB)\n",
            "\u001b[K     |████████████████████████████████| 556 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting hjson\n",
            "  Downloading hjson-3.0.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed) (21.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed) (5.4.8)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.64.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deepspeed) (4.2.0)\n",
            "Building wheels for collected packages: deepspeed, py-cpuinfo\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.6.4-py3-none-any.whl size=556303 sha256=e11fb170094b2ea22febb6793603ae641ac2151c8bea00e2d2d7928167456c7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/51/5a/aa1287f4e6ea5f57027e352761c0ae0a7cf0a300f50c45e68c\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=641bbea2b09ebc9b846dd17f8240b12238d8445fe6219697fff76f7290cf6e03\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built deepspeed py-cpuinfo\n",
            "Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n",
            "Successfully installed deepspeed-0.6.4 hjson-3.0.2 ninja-1.10.2.3 py-cpuinfo-8.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run-ds.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJUR2izdzV_B",
        "outputId": "e84a3606-5f58-4930-e892-4078b8839cd9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"data\": \"dataset1\",\n",
            "  \"seq_len\": 96,\n",
            "  \"pred_len\": 24,\n",
            "  \"dec_seq_len\": 48,\n",
            "  \"hidden_size\": 128,\n",
            "  \"heads\": 5,\n",
            "  \"n_encoder_layers\": 2,\n",
            "  \"encoder_attention\": \"query_selector_0.5\",\n",
            "  \"n_decoder_layers\": 2,\n",
            "  \"decoder_attention\": \"full\",\n",
            "  \"batch_size\": 64,\n",
            "  \"embedding_size\": 48,\n",
            "  \"prediction_type\": \"uni\",\n",
            "  \"dropout\": 0.05,\n",
            "  \"fp16\": true,\n",
            "  \"deepspeed\": true,\n",
            "  \"iterations\": 5,\n",
            "  \"exps\": 1,\n",
            "  \"debug\": false\n",
            "}\n",
            "[2022-05-08 11:20:03,743] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2022-05-08 11:20:03,775] [INFO] [runner.py:453:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 train.py --deepspeed_config settings/ds_config_zero.json --data dataset1 --seq_len 96 --pred_len 24 --dec_seq_len 48 --hidden_size 128 --n_encoder_layers 2 --n_decoder_layers 2 --encoder_attention query_selector_0.5 --decoder_attention full --n_heads 5 --batch_size 64 --embedding_size 48 --iterations 5 --exps 1 --dropout 0.05 --fp16 --deepspeed --features S --input_len 1 --output_len 1 --run_num 1\n",
            "[2022-05-08 11:20:05,280] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.7.8\n",
            "[2022-05-08 11:20:05,280] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2022-05-08 11:20:05,280] [INFO] [launch.py:110:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2022-05-08 11:20:05,280] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2022-05-08 11:20:05,280] [INFO] [launch.py:123:main] dist_world_size=1\n",
            "[2022-05-08 11:20:05,280] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "Number of parameters: 138\n",
            "[2022-05-08 11:20:11,133] [INFO] [logging.py:69:log_dist] [Rank -1] DeepSpeed info: version=0.6.4, git-hash=unknown, git-branch=unknown\n",
            "[2022-05-08 11:20:11,135] [INFO] [distributed.py:49:init_distributed] Initializing torch distributed with backend: nccl\n",
            "[2022-05-08 11:20:20,399] [INFO] [engine.py:279:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "Installed CUDA version 11.1 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py37_cu113/fused_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu113/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
            "[2/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_37,code=compute_37 -gencode=arch=compute_37,code=sm_37 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_37,code=compute_37 -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 39.32983088493347 seconds\n",
            "[2022-05-08 11:21:00,494] [INFO] [engine.py:1060:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2022-05-08 11:21:00,499] [INFO] [engine.py:1067:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2022-05-08 11:21:00,499] [INFO] [utils.py:53:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
            "[2022-05-08 11:21:00,499] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
            "[2022-05-08 11:21:00,499] [INFO] [stage_1_and_2.py:132:__init__] Reduce bucket size 500000000\n",
            "[2022-05-08 11:21:00,499] [INFO] [stage_1_and_2.py:133:__init__] Allgather bucket size 500000000\n",
            "[2022-05-08 11:21:00,500] [INFO] [stage_1_and_2.py:134:__init__] CPU Offload: False\n",
            "[2022-05-08 11:21:00,500] [INFO] [stage_1_and_2.py:135:__init__] Round robin gradient partitioning: False\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py37_cu113/utils...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu113/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 18.220255851745605 seconds\n",
            "Rank: 0 partition count [1] and sizes[(582168, False)] \n",
            "[2022-05-08 11:21:18,860] [INFO] [utils.py:828:see_memory_usage] Before initializing optimizer states\n",
            "[2022-05-08 11:21:18,861] [INFO] [utils.py:833:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.02 GB         Max_CA 0 GB \n",
            "[2022-05-08 11:21:18,861] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 2.36 GB, percent = 18.6%\n",
            "[2022-05-08 11:21:18,925] [INFO] [utils.py:828:see_memory_usage] After initializing optimizer states\n",
            "[2022-05-08 11:21:18,926] [INFO] [utils.py:833:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 0.02 GB         Max_CA 0 GB \n",
            "[2022-05-08 11:21:18,927] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 2.36 GB, percent = 18.6%\n",
            "[2022-05-08 11:21:18,927] [INFO] [stage_1_and_2.py:507:__init__] optimizer state initialized\n",
            "[2022-05-08 11:21:18,978] [INFO] [utils.py:828:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2022-05-08 11:21:18,980] [INFO] [utils.py:833:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 0.02 GB         Max_CA 0 GB \n",
            "[2022-05-08 11:21:18,981] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 2.36 GB, percent = 18.6%\n",
            "[2022-05-08 11:21:18,981] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
            "[2022-05-08 11:21:18,981] [INFO] [engine.py:776:_configure_lr_scheduler] DeepSpeed using client LR scheduler\n",
            "[2022-05-08 11:21:18,981] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2022-05-08 11:21:18,982] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:18,983] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
            "[2022-05-08 11:21:18,983] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2022-05-08 11:21:18,983] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2022-05-08 11:21:18,983] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
            "[2022-05-08 11:21:18,983] [INFO] [config.py:1063:print]   amp_params ................... False\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": null, \n",
            "    \"exps_dir\": null, \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
            "[2022-05-08 11:21:18,984] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   dump_state ................... False\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
            "[2022-05-08 11:21:18,985] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   gradient_clipping ............ 0.0\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 5e-05, 'weight_decay': 0.01}\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
            "[2022-05-08 11:21:18,986] [INFO] [config.py:1063:print]   pld_params ................... False\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   scheduler_name ............... None\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   scheduler_params ............. None\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
            "[2022-05-08 11:21:18,987] [INFO] [config.py:1063:print]   steps_per_print .............. 10\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   train_batch_size ............. 5\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  5\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   world_size ................... 1\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   zero_config .................. {\n",
            "    \"stage\": 2, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 5.000000e+08, \n",
            "    \"allgather_partitions\": false, \n",
            "    \"allgather_bucket_size\": 5.000000e+08, \n",
            "    \"overlap_comm\": false, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": false, \n",
            "    \"offload_param\": null, \n",
            "    \"offload_optimizer\": null, \n",
            "    \"sub_group_size\": 1.000000e+09, \n",
            "    \"prefetch_bucket_size\": 5.000000e+07, \n",
            "    \"param_persistence_threshold\": 1.000000e+05, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_16bit_weights_on_model_save\": false, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"round_robin_gradients\": false, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   zero_enabled ................. True\n",
            "[2022-05-08 11:21:18,988] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 2\n",
            "[2022-05-08 11:21:18,989] [INFO] [config.py:1071:print]   json = {\n",
            "    \"train_micro_batch_size_per_gpu\": 5, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"steps_per_print\": 10, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"Adam\", \n",
            "        \"params\": {\n",
            "            \"lr\": 5e-05, \n",
            "            \"weight_decay\": 0.01\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"allgather_partitions\": false, \n",
            "        \"cpu_offload\": false\n",
            "    }, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale_window\": 1000\n",
            "    }\n",
            "}\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0004296302795410156 seconds\n",
            "train 2651\n",
            " Run   1, iteration:   1:   Loss at step 1: 1.2666015625, mean for epoch: 1.2666015625, mem_alloc: 140285952\n",
            "[2022-05-08 11:21:20,640] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296\n",
            " Run   1, iteration:   1:   Loss at step 2: 1.2333984375, mean for epoch: 1.25, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:21,217] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n",
            " Run   1, iteration:   1:   Loss at step 3: 1.1708984375, mean for epoch: 1.2236328125, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:21,784] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n",
            " Run   1, iteration:   1:   Loss at step 4: 1.296875, mean for epoch: 1.241943359375, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:22,354] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n",
            " Run   1, iteration:   1:   Loss at step 5: 1.3623046875, mean for epoch: 1.266015625, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:22,923] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n",
            " Run   1, iteration:   1:   Loss at step 6: 1.26953125, mean for epoch: 1.2666015625, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:23,494] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\n",
            " Run   1, iteration:   1:   Loss at step 7: 1.1376953125, mean for epoch: 1.2481863839285714, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:24,065] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\n",
            " Run   1, iteration:   1:   Loss at step 8: 1.134765625, mean for epoch: 1.2340087890625, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:24,634] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0\n",
            " Run   1, iteration:   1:   Loss at step 9: 1.0673828125, mean for epoch: 1.2154947916666667, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:25,204] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0\n",
            " Run   1, iteration:   1:   Loss at step 10: 1.2880859375, mean for epoch: 1.22275390625, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:25,774] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0\n",
            "[2022-05-08 11:21:25,775] [INFO] [logging.py:69:log_dist] [Rank 0] step=10, skipped=10, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:25,775] [INFO] [timer.py:201:stop] 0/10, SamplesPerSec=8.829651289314059, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   1:   Loss at step 11: 1.06640625, mean for epoch: 1.2085404829545454, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:26,342] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0\n",
            " Run   1, iteration:   1:   Loss at step 12: 1.58984375, mean for epoch: 1.2403157552083333, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:26,909] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0\n",
            " Run   1, iteration:   1:   Loss at step 13: 1.1669921875, mean for epoch: 1.2346754807692308, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:27,479] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0\n",
            " Run   1, iteration:   1:   Loss at step 14: 1.3916015625, mean for epoch: 1.2458844866071428, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:28,047] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n",
            " Run   1, iteration:   1:   Loss at step 15: 1.2294921875, mean for epoch: 1.2447916666666667, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:28,617] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0\n",
            " Run   1, iteration:   1:   Loss at step 16: 1.306640625, mean for epoch: 1.2486572265625, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:29,188] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n",
            " Run   1, iteration:   1:   Loss at step 17: 1.2109375, mean for epoch: 1.246438419117647, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:29,756] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0\n",
            " Run   1, iteration:   1:   Loss at step 18: 1.3486328125, mean for epoch: 1.2521158854166667, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:30,325] [INFO] [stage_1_and_2.py:1655:step] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n",
            " Run   1, iteration:   1:   Loss at step 19: 1.4677734375, mean for epoch: 1.263466282894737, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 20: 1.20703125, mean for epoch: 1.26064453125, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:31,515] [INFO] [logging.py:69:log_dist] [Rank 0] step=20, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:31,516] [INFO] [timer.py:201:stop] 0/20, SamplesPerSec=8.79518763407868, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   1:   Loss at step 21: 1.28515625, mean for epoch: 1.261811755952381, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 22: 1.0029296875, mean for epoch: 1.2500443892045454, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 23: 1.0185546875, mean for epoch: 1.2399796195652173, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 24: 0.8515625, mean for epoch: 1.2237955729166667, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 25: 0.97802734375, mean for epoch: 1.21396484375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 26: 0.84326171875, mean for epoch: 1.19970703125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 27: 0.88037109375, mean for epoch: 1.1878797743055556, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 28: 0.7333984375, mean for epoch: 1.1716482979910714, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 29: 0.81298828125, mean for epoch: 1.1592807112068966, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 30: 0.71826171875, mean for epoch: 1.144580078125, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:37,416] [INFO] [logging.py:69:log_dist] [Rank 0] step=30, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:37,417] [INFO] [timer.py:201:stop] 0/30, SamplesPerSec=8.701130352218355, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   1:   Loss at step 31: 0.69189453125, mean for epoch: 1.129977318548387, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 32: 0.65185546875, mean for epoch: 1.1150360107421875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 33: 0.72216796875, mean for epoch: 1.103130918560606, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 34: 0.63134765625, mean for epoch: 1.089254940257353, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 35: 0.63037109375, mean for epoch: 1.0761439732142857, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 36: 0.646484375, mean for epoch: 1.064208984375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 37: 0.640625, mean for epoch: 1.0527607685810811, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 38: 0.6171875, mean for epoch: 1.041298314144737, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 39: 0.5869140625, mean for epoch: 1.029647435897436, mem_alloc: 1140746752\n",
            " Run   1, iteration:   1:   Loss at step 40: 0.5576171875, mean for epoch: 1.0178466796875, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:43,337] [INFO] [logging.py:69:log_dist] [Rank 0] step=40, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:43,338] [INFO] [timer.py:201:stop] 0/40, SamplesPerSec=8.648376273968378, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   1:   Loss at step 41: 0.50341796875, mean for epoch: 1.0052996379573171, mem_alloc: 1140746752\n",
            "test shape: (2624, 24, 1) (2624, 24, 1)\n",
            "Loss after iteration 1 ; MSE: 1.0048828125, MAE: 0.7529296875\n",
            "Connected by ('127.0.0.1', 44062)\n",
            "\u001b[94mReceived training result: b'1;1.00488;0.75293' \u001b[0m\n",
            "Time per iteration 24.124825954437256, memory OrderedDict([('active.all.allocated', 62786), ('active.all.current', 7), ('active.all.freed', 62779), ('active.all.peak', 236), ('active.large_pool.allocated', 4749), ('active.large_pool.current', 5), ('active.large_pool.freed', 4744), ('active.large_pool.peak', 32), ('active.small_pool.allocated', 58037), ('active.small_pool.current', 2), ('active.small_pool.freed', 58035), ('active.small_pool.peak', 205), ('active_bytes.all.allocated', 65971257344), ('active_bytes.all.current', 9797632), ('active_bytes.all.freed', 65961459712), ('active_bytes.all.peak', 1140746752), ('active_bytes.large_pool.allocated', 50009474560), ('active_bytes.large_pool.current', 9316864), ('active_bytes.large_pool.freed', 50000157696), ('active_bytes.large_pool.peak', 1055092224), ('active_bytes.small_pool.allocated', 15961782784), ('active_bytes.small_pool.current', 480768), ('active_bytes.small_pool.freed', 15961302016), ('active_bytes.small_pool.peak', 87227392), ('allocated_bytes.all.allocated', 65971257344), ('allocated_bytes.all.current', 8632832), ('allocated_bytes.all.freed', 65962624512), ('allocated_bytes.all.peak', 1140746752), ('allocated_bytes.large_pool.allocated', 50009474560), ('allocated_bytes.large_pool.current', 8152064), ('allocated_bytes.large_pool.freed', 50001322496), ('allocated_bytes.large_pool.peak', 1055092224), ('allocated_bytes.small_pool.allocated', 15961782784), ('allocated_bytes.small_pool.current', 480768), ('allocated_bytes.small_pool.freed', 15961302016), ('allocated_bytes.small_pool.peak', 87227392), ('allocation.all.allocated', 62786), ('allocation.all.current', 6), ('allocation.all.freed', 62780), ('allocation.all.peak', 236), ('allocation.large_pool.allocated', 4749), ('allocation.large_pool.current', 4), ('allocation.large_pool.freed', 4745), ('allocation.large_pool.peak', 32), ('allocation.small_pool.allocated', 58037), ('allocation.small_pool.current', 2), ('allocation.small_pool.freed', 58035), ('allocation.small_pool.peak', 205), ('inactive_split.all.allocated', 25015), ('inactive_split.all.current', 5), ('inactive_split.all.freed', 25010), ('inactive_split.all.peak', 58), ('inactive_split.large_pool.allocated', 2157), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 2155), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 22858), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 22855), ('inactive_split.small_pool.peak', 54), ('inactive_split_bytes.all.allocated', 29523957248), ('inactive_split_bytes.all.current', 13271040), ('inactive_split_bytes.all.freed', 29510686208), ('inactive_split_bytes.all.peak', 34571264), ('inactive_split_bytes.large_pool.allocated', 11562620416), ('inactive_split_bytes.large_pool.current', 11654656), ('inactive_split_bytes.large_pool.freed', 11550965760), ('inactive_split_bytes.large_pool.peak', 29268992), ('inactive_split_bytes.small_pool.allocated', 17961336832), ('inactive_split_bytes.small_pool.current', 1616384), ('inactive_split_bytes.small_pool.freed', 17959720448), ('inactive_split_bytes.small_pool.peak', 9044480), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('reserved_bytes.all.allocated', 1155530752), ('reserved_bytes.all.current', 1155530752), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 1155530752), ('reserved_bytes.large_pool.allocated', 1063256064), ('reserved_bytes.large_pool.current', 1063256064), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 1063256064), ('reserved_bytes.small_pool.allocated', 92274688), ('reserved_bytes.small_pool.current', 92274688), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 92274688), ('segment.all.allocated', 48), ('segment.all.current', 48), ('segment.all.freed', 0), ('segment.all.peak', 48), ('segment.large_pool.allocated', 4), ('segment.large_pool.current', 4), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 4), ('segment.small_pool.allocated', 44), ('segment.small_pool.current', 44), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 44)])\n",
            " Run   1, iteration:   2:   Loss at step 1: 0.407958984375, mean for epoch: 0.407958984375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 2: 0.4677734375, mean for epoch: 0.4378662109375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 3: 0.401611328125, mean for epoch: 0.42578125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 4: 0.471435546875, mean for epoch: 0.43719482421875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 5: 0.57080078125, mean for epoch: 0.463916015625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 6: 0.53955078125, mean for epoch: 0.4765218098958333, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 7: 0.42236328125, mean for epoch: 0.46878487723214285, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 8: 0.5263671875, mean for epoch: 0.475982666015625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 9: 0.48681640625, mean for epoch: 0.4771864149305556, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:49,344] [INFO] [logging.py:69:log_dist] [Rank 0] step=50, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:49,345] [INFO] [timer.py:201:stop] 0/50, SamplesPerSec=8.61983544497116, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   2:   Loss at step 10: 0.415771484375, mean for epoch: 0.471044921875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 11: 0.47216796875, mean for epoch: 0.47114701704545453, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 12: 0.430908203125, mean for epoch: 0.4677937825520833, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 13: 0.439453125, mean for epoch: 0.46561373197115385, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 14: 0.47802734375, mean for epoch: 0.4665004185267857, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 15: 0.431884765625, mean for epoch: 0.4641927083333333, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 16: 0.408447265625, mean for epoch: 0.4607086181640625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 17: 0.28564453125, mean for epoch: 0.45041073069852944, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 18: 0.408935546875, mean for epoch: 0.4481065538194444, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 19: 0.355224609375, mean for epoch: 0.4432180304276316, mem_alloc: 1140746752\n",
            "[2022-05-08 11:21:55,274] [INFO] [logging.py:69:log_dist] [Rank 0] step=60, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:21:55,274] [INFO] [timer.py:201:stop] 0/60, SamplesPerSec=8.598075501304223, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   2:   Loss at step 20: 0.52099609375, mean for epoch: 0.44710693359375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 21: 0.380126953125, mean for epoch: 0.4439174107142857, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 22: 0.234130859375, mean for epoch: 0.4343816583806818, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 23: 0.385498046875, mean for epoch: 0.4322562839673913, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 24: 0.361083984375, mean for epoch: 0.429290771484375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 25: 0.36669921875, mean for epoch: 0.426787109375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 26: 0.3349609375, mean for epoch: 0.42325533353365385, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 27: 0.3740234375, mean for epoch: 0.42143192997685186, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 28: 0.310791015625, mean for epoch: 0.41748046875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 29: 0.326416015625, mean for epoch: 0.4143403151939655, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:01,203] [INFO] [logging.py:69:log_dist] [Rank 0] step=70, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:01,204] [INFO] [timer.py:201:stop] 0/70, SamplesPerSec=8.581689037020688, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   2:   Loss at step 30: 0.402099609375, mean for epoch: 0.4139322916666667, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 31: 0.261962890625, mean for epoch: 0.4090300529233871, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 32: 0.3798828125, mean for epoch: 0.40811920166015625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 33: 0.39892578125, mean for epoch: 0.4078406131628788, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 34: 0.454833984375, mean for epoch: 0.4092227711397059, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 35: 0.30517578125, mean for epoch: 0.40625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 36: 0.274169921875, mean for epoch: 0.4025811089409722, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 37: 0.406494140625, mean for epoch: 0.40268686655405406, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 38: 0.27734375, mean for epoch: 0.3993883634868421, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 39: 0.27783203125, mean for epoch: 0.3962715344551282, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:07,125] [INFO] [logging.py:69:log_dist] [Rank 0] step=80, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:07,126] [INFO] [timer.py:201:stop] 0/80, SamplesPerSec=8.570693796172513, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   2:   Loss at step 40: 0.395263671875, mean for epoch: 0.396246337890625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   2:   Loss at step 41: 0.365478515625, mean for epoch: 0.3954959032012195, mem_alloc: 1140746752\n",
            "test shape: (2624, 24, 1) (2624, 24, 1)\n",
            "Loss after iteration 2 ; MSE: 0.3955078125, MAE: 0.43115234375\n",
            "Connected by ('127.0.0.1', 44166)\n",
            "\u001b[94mReceived training result: b'2;0.39551;0.43115' \u001b[0m\n",
            "Time per iteration 24.253933548927307, memory OrderedDict([('active.all.allocated', 135397), ('active.all.current', 7), ('active.all.freed', 135390), ('active.all.peak', 236), ('active.large_pool.allocated', 9546), ('active.large_pool.current', 5), ('active.large_pool.freed', 9541), ('active.large_pool.peak', 32), ('active.small_pool.allocated', 125851), ('active.small_pool.current', 2), ('active.small_pool.freed', 125849), ('active.small_pool.peak', 205), ('active_bytes.all.allocated', 132141463040), ('active_bytes.all.current', 9797632), ('active_bytes.all.freed', 132131665408), ('active_bytes.all.peak', 1140746752), ('active_bytes.large_pool.allocated', 100091159552), ('active_bytes.large_pool.current', 9316864), ('active_bytes.large_pool.freed', 100081842688), ('active_bytes.large_pool.peak', 1055092224), ('active_bytes.small_pool.allocated', 32050303488), ('active_bytes.small_pool.current', 480768), ('active_bytes.small_pool.freed', 32049822720), ('active_bytes.small_pool.peak', 87227392), ('allocated_bytes.all.allocated', 132141463040), ('allocated_bytes.all.current', 8632832), ('allocated_bytes.all.freed', 132132830208), ('allocated_bytes.all.peak', 1140746752), ('allocated_bytes.large_pool.allocated', 100091159552), ('allocated_bytes.large_pool.current', 8152064), ('allocated_bytes.large_pool.freed', 100083007488), ('allocated_bytes.large_pool.peak', 1055092224), ('allocated_bytes.small_pool.allocated', 32050303488), ('allocated_bytes.small_pool.current', 480768), ('allocated_bytes.small_pool.freed', 32049822720), ('allocated_bytes.small_pool.peak', 87227392), ('allocation.all.allocated', 135397), ('allocation.all.current', 6), ('allocation.all.freed', 135391), ('allocation.all.peak', 236), ('allocation.large_pool.allocated', 9546), ('allocation.large_pool.current', 4), ('allocation.large_pool.freed', 9542), ('allocation.large_pool.peak', 32), ('allocation.small_pool.allocated', 125851), ('allocation.small_pool.current', 2), ('allocation.small_pool.freed', 125849), ('allocation.small_pool.peak', 205), ('inactive_split.all.allocated', 53210), ('inactive_split.all.current', 5), ('inactive_split.all.freed', 53205), ('inactive_split.all.peak', 58), ('inactive_split.large_pool.allocated', 4330), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 4328), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 48880), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 48877), ('inactive_split.small_pool.peak', 54), ('inactive_split_bytes.all.allocated', 59234761216), ('inactive_split_bytes.all.current', 13271040), ('inactive_split_bytes.all.freed', 59221490176), ('inactive_split_bytes.all.peak', 34571264), ('inactive_split_bytes.large_pool.allocated', 23186961408), ('inactive_split_bytes.large_pool.current', 11654656), ('inactive_split_bytes.large_pool.freed', 23175306752), ('inactive_split_bytes.large_pool.peak', 29268992), ('inactive_split_bytes.small_pool.allocated', 36047799808), ('inactive_split_bytes.small_pool.current', 1616384), ('inactive_split_bytes.small_pool.freed', 36046183424), ('inactive_split_bytes.small_pool.peak', 9044480), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('reserved_bytes.all.allocated', 1155530752), ('reserved_bytes.all.current', 1155530752), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 1155530752), ('reserved_bytes.large_pool.allocated', 1063256064), ('reserved_bytes.large_pool.current', 1063256064), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 1063256064), ('reserved_bytes.small_pool.allocated', 92274688), ('reserved_bytes.small_pool.current', 92274688), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 92274688), ('segment.all.allocated', 48), ('segment.all.current', 48), ('segment.all.freed', 0), ('segment.all.peak', 48), ('segment.large_pool.allocated', 4), ('segment.large_pool.current', 4), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 4), ('segment.small_pool.allocated', 44), ('segment.small_pool.current', 44), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 44)])\n",
            " Run   1, iteration:   3:   Loss at step 1: 0.2607421875, mean for epoch: 0.2607421875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 2: 0.352783203125, mean for epoch: 0.3067626953125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 3: 0.296142578125, mean for epoch: 0.30322265625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 4: 0.419189453125, mean for epoch: 0.33221435546875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 5: 0.360595703125, mean for epoch: 0.337890625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 6: 0.2607421875, mean for epoch: 0.3250325520833333, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 7: 0.295166015625, mean for epoch: 0.32076590401785715, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 8: 0.2548828125, mean for epoch: 0.312530517578125, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:13,150] [INFO] [logging.py:69:log_dist] [Rank 0] step=90, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:13,151] [INFO] [timer.py:201:stop] 0/90, SamplesPerSec=8.561286780552408, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   3:   Loss at step 9: 0.275634765625, mean for epoch: 0.3084309895833333, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 10: 0.3681640625, mean for epoch: 0.314404296875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 11: 0.292236328125, mean for epoch: 0.31238902698863635, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 12: 0.296142578125, mean for epoch: 0.31103515625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 13: 0.296142578125, mean for epoch: 0.3098895733173077, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 14: 0.240234375, mean for epoch: 0.30491420200892855, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 15: 0.307373046875, mean for epoch: 0.305078125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 16: 0.218505859375, mean for epoch: 0.2996673583984375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 17: 0.291259765625, mean for epoch: 0.2991727941176471, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 18: 0.3623046875, mean for epoch: 0.3026801215277778, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:19,049] [INFO] [logging.py:69:log_dist] [Rank 0] step=100, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:19,050] [INFO] [timer.py:201:stop] 0/100, SamplesPerSec=8.558727057978636, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   3:   Loss at step 19: 0.260986328125, mean for epoch: 0.30048571134868424, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 20: 0.31640625, mean for epoch: 0.30128173828125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 21: 0.267578125, mean for epoch: 0.29967680431547616, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 22: 0.2120361328125, mean for epoch: 0.2956931374289773, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 23: 0.32373046875, mean for epoch: 0.29691215183423914, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 24: 0.362548828125, mean for epoch: 0.2996470133463542, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 25: 0.2646484375, mean for epoch: 0.2982470703125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 26: 0.249755859375, mean for epoch: 0.2963820237379808, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 27: 0.2313232421875, mean for epoch: 0.2939724392361111, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 28: 0.2220458984375, mean for epoch: 0.2914036342075893, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:24,970] [INFO] [logging.py:69:log_dist] [Rank 0] step=110, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:24,971] [INFO] [timer.py:201:stop] 0/110, SamplesPerSec=8.553584920678196, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   3:   Loss at step 29: 0.29052734375, mean for epoch: 0.2913734172952586, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 30: 0.32958984375, mean for epoch: 0.29264729817708335, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 31: 0.2421875, mean for epoch: 0.2910195627520161, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 32: 0.273681640625, mean for epoch: 0.2904777526855469, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 33: 0.2181396484375, mean for epoch: 0.28828568892045453, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 34: 0.26806640625, mean for epoch: 0.28769100413602944, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 35: 0.215576171875, mean for epoch: 0.28563058035714284, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 36: 0.2095947265625, mean for epoch: 0.2835184733072917, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 37: 0.2177734375, mean for epoch: 0.28174158044763514, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 38: 0.22412109375, mean for epoch: 0.2802252518503289, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:30,893] [INFO] [logging.py:69:log_dist] [Rank 0] step=120, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:30,894] [INFO] [timer.py:201:stop] 0/120, SamplesPerSec=8.548505329051249, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   3:   Loss at step 39: 0.27294921875, mean for epoch: 0.28003868689903844, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 40: 0.253662109375, mean for epoch: 0.2793792724609375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   3:   Loss at step 41: 0.1854248046875, mean for epoch: 0.2770877000762195, mem_alloc: 1140746752\n",
            "test shape: (2624, 24, 1) (2624, 24, 1)\n",
            "Loss after iteration 3 ; MSE: 0.277099609375, MAE: 0.3349609375\n",
            "Connected by ('127.0.0.1', 44260)\n",
            "\u001b[94mReceived training result: b'3;0.27710;0.33496' \u001b[0m\n",
            "Time per iteration 24.295816659927368, memory OrderedDict([('active.all.allocated', 208008), ('active.all.current', 7), ('active.all.freed', 208001), ('active.all.peak', 236), ('active.large_pool.allocated', 14343), ('active.large_pool.current', 5), ('active.large_pool.freed', 14338), ('active.large_pool.peak', 32), ('active.small_pool.allocated', 193665), ('active.small_pool.current', 2), ('active.small_pool.freed', 193663), ('active.small_pool.peak', 205), ('active_bytes.all.allocated', 198311668736), ('active_bytes.all.current', 9797632), ('active_bytes.all.freed', 198301871104), ('active_bytes.all.peak', 1140746752), ('active_bytes.large_pool.allocated', 150172844544), ('active_bytes.large_pool.current', 9316864), ('active_bytes.large_pool.freed', 150163527680), ('active_bytes.large_pool.peak', 1055092224), ('active_bytes.small_pool.allocated', 48138824192), ('active_bytes.small_pool.current', 480768), ('active_bytes.small_pool.freed', 48138343424), ('active_bytes.small_pool.peak', 87227392), ('allocated_bytes.all.allocated', 198311668736), ('allocated_bytes.all.current', 8632832), ('allocated_bytes.all.freed', 198303035904), ('allocated_bytes.all.peak', 1140746752), ('allocated_bytes.large_pool.allocated', 150172844544), ('allocated_bytes.large_pool.current', 8152064), ('allocated_bytes.large_pool.freed', 150164692480), ('allocated_bytes.large_pool.peak', 1055092224), ('allocated_bytes.small_pool.allocated', 48138824192), ('allocated_bytes.small_pool.current', 480768), ('allocated_bytes.small_pool.freed', 48138343424), ('allocated_bytes.small_pool.peak', 87227392), ('allocation.all.allocated', 208008), ('allocation.all.current', 6), ('allocation.all.freed', 208002), ('allocation.all.peak', 236), ('allocation.large_pool.allocated', 14343), ('allocation.large_pool.current', 4), ('allocation.large_pool.freed', 14339), ('allocation.large_pool.peak', 32), ('allocation.small_pool.allocated', 193665), ('allocation.small_pool.current', 2), ('allocation.small_pool.freed', 193663), ('allocation.small_pool.peak', 205), ('inactive_split.all.allocated', 81405), ('inactive_split.all.current', 5), ('inactive_split.all.freed', 81400), ('inactive_split.all.peak', 58), ('inactive_split.large_pool.allocated', 6503), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 6501), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 74902), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 74899), ('inactive_split.small_pool.peak', 54), ('inactive_split_bytes.all.allocated', 88945565184), ('inactive_split_bytes.all.current', 13271040), ('inactive_split_bytes.all.freed', 88932294144), ('inactive_split_bytes.all.peak', 34571264), ('inactive_split_bytes.large_pool.allocated', 34811302400), ('inactive_split_bytes.large_pool.current', 11654656), ('inactive_split_bytes.large_pool.freed', 34799647744), ('inactive_split_bytes.large_pool.peak', 29268992), ('inactive_split_bytes.small_pool.allocated', 54134262784), ('inactive_split_bytes.small_pool.current', 1616384), ('inactive_split_bytes.small_pool.freed', 54132646400), ('inactive_split_bytes.small_pool.peak', 9044480), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('reserved_bytes.all.allocated', 1155530752), ('reserved_bytes.all.current', 1155530752), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 1155530752), ('reserved_bytes.large_pool.allocated', 1063256064), ('reserved_bytes.large_pool.current', 1063256064), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 1063256064), ('reserved_bytes.small_pool.allocated', 92274688), ('reserved_bytes.small_pool.current', 92274688), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 92274688), ('segment.all.allocated', 48), ('segment.all.current', 48), ('segment.all.freed', 0), ('segment.all.peak', 48), ('segment.large_pool.allocated', 4), ('segment.large_pool.current', 4), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 4), ('segment.small_pool.allocated', 44), ('segment.small_pool.current', 44), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 44)])\n",
            " Run   1, iteration:   4:   Loss at step 1: 0.30419921875, mean for epoch: 0.30419921875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 2: 0.18994140625, mean for epoch: 0.2470703125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 3: 0.21240234375, mean for epoch: 0.23551432291666666, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 4: 0.276123046875, mean for epoch: 0.24566650390625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 5: 0.241943359375, mean for epoch: 0.244921875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 6: 0.259033203125, mean for epoch: 0.24727376302083334, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 7: 0.2449951171875, mean for epoch: 0.2469482421875, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:36,926] [INFO] [logging.py:69:log_dist] [Rank 0] step=130, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:36,926] [INFO] [timer.py:201:stop] 0/130, SamplesPerSec=8.544070024110574, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   4:   Loss at step 8: 0.286376953125, mean for epoch: 0.2518768310546875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 9: 0.23876953125, mean for epoch: 0.2504204644097222, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 10: 0.228271484375, mean for epoch: 0.24820556640625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 11: 0.252685546875, mean for epoch: 0.24861283735795456, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 12: 0.236328125, mean for epoch: 0.247589111328125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 13: 0.1861572265625, mean for epoch: 0.24286358173076922, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 14: 0.2252197265625, mean for epoch: 0.24160330636160715, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 15: 0.155517578125, mean for epoch: 0.2358642578125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 16: 0.2091064453125, mean for epoch: 0.23419189453125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 17: 0.199462890625, mean for epoch: 0.2321490119485294, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:42,840] [INFO] [logging.py:69:log_dist] [Rank 0] step=140, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:42,840] [INFO] [timer.py:201:stop] 0/140, SamplesPerSec=8.541424910792294, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   4:   Loss at step 18: 0.1973876953125, mean for epoch: 0.2302178276909722, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 19: 0.2353515625, mean for epoch: 0.23048802425986842, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 20: 0.2086181640625, mean for epoch: 0.22939453125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 21: 0.14599609375, mean for epoch: 0.22542317708333334, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 22: 0.1868896484375, mean for epoch: 0.22367165305397727, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 23: 0.1702880859375, mean for epoch: 0.22135062839673914, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 24: 0.1668701171875, mean for epoch: 0.21908060709635416, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 25: 0.21337890625, mean for epoch: 0.2188525390625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 26: 0.1512451171875, mean for epoch: 0.21625225360576922, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 27: 0.19384765625, mean for epoch: 0.2154224537037037, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:48,768] [INFO] [logging.py:69:log_dist] [Rank 0] step=150, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:48,768] [INFO] [timer.py:201:stop] 0/150, SamplesPerSec=8.537787515197572, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   4:   Loss at step 28: 0.1275634765625, mean for epoch: 0.21228463309151785, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 29: 0.177734375, mean for epoch: 0.21109324488146552, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 30: 0.195068359375, mean for epoch: 0.21055908203125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 31: 0.220947265625, mean for epoch: 0.2108941847278226, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 32: 0.162109375, mean for epoch: 0.20936965942382812, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 33: 0.25390625, mean for epoch: 0.21071925307765152, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 34: 0.1915283203125, mean for epoch: 0.21015481387867646, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 35: 0.1982421875, mean for epoch: 0.209814453125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 36: 0.2056884765625, mean for epoch: 0.20969984266493055, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 37: 0.136962890625, mean for epoch: 0.20773397909628377, mem_alloc: 1140746752\n",
            "[2022-05-08 11:22:54,687] [INFO] [logging.py:69:log_dist] [Rank 0] step=160, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:22:54,687] [INFO] [timer.py:201:stop] 0/160, SamplesPerSec=8.535655607201859, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   4:   Loss at step 38: 0.1580810546875, mean for epoch: 0.20642732319078946, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 39: 0.1702880859375, mean for epoch: 0.20550067608173078, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 40: 0.1617431640625, mean for epoch: 0.20440673828125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   4:   Loss at step 41: 0.2103271484375, mean for epoch: 0.20455113852896342, mem_alloc: 1140746752\n",
            "test shape: (2624, 24, 1) (2624, 24, 1)\n",
            "Loss after iteration 4 ; MSE: 0.20458984375, MAE: 0.266357421875\n",
            "Connected by ('127.0.0.1', 44374)\n",
            "\u001b[94mReceived training result: b'4;0.20459;0.26636' \u001b[0m\n",
            "Time per iteration 24.310693681240082, memory OrderedDict([('active.all.allocated', 280619), ('active.all.current', 7), ('active.all.freed', 280612), ('active.all.peak', 236), ('active.large_pool.allocated', 19140), ('active.large_pool.current', 5), ('active.large_pool.freed', 19135), ('active.large_pool.peak', 32), ('active.small_pool.allocated', 261479), ('active.small_pool.current', 2), ('active.small_pool.freed', 261477), ('active.small_pool.peak', 205), ('active_bytes.all.allocated', 264481874432), ('active_bytes.all.current', 9797632), ('active_bytes.all.freed', 264472076800), ('active_bytes.all.peak', 1140746752), ('active_bytes.large_pool.allocated', 200254529536), ('active_bytes.large_pool.current', 9316864), ('active_bytes.large_pool.freed', 200245212672), ('active_bytes.large_pool.peak', 1055092224), ('active_bytes.small_pool.allocated', 64227344896), ('active_bytes.small_pool.current', 480768), ('active_bytes.small_pool.freed', 64226864128), ('active_bytes.small_pool.peak', 87227392), ('allocated_bytes.all.allocated', 264481874432), ('allocated_bytes.all.current', 8632832), ('allocated_bytes.all.freed', 264473241600), ('allocated_bytes.all.peak', 1140746752), ('allocated_bytes.large_pool.allocated', 200254529536), ('allocated_bytes.large_pool.current', 8152064), ('allocated_bytes.large_pool.freed', 200246377472), ('allocated_bytes.large_pool.peak', 1055092224), ('allocated_bytes.small_pool.allocated', 64227344896), ('allocated_bytes.small_pool.current', 480768), ('allocated_bytes.small_pool.freed', 64226864128), ('allocated_bytes.small_pool.peak', 87227392), ('allocation.all.allocated', 280619), ('allocation.all.current', 6), ('allocation.all.freed', 280613), ('allocation.all.peak', 236), ('allocation.large_pool.allocated', 19140), ('allocation.large_pool.current', 4), ('allocation.large_pool.freed', 19136), ('allocation.large_pool.peak', 32), ('allocation.small_pool.allocated', 261479), ('allocation.small_pool.current', 2), ('allocation.small_pool.freed', 261477), ('allocation.small_pool.peak', 205), ('inactive_split.all.allocated', 109600), ('inactive_split.all.current', 5), ('inactive_split.all.freed', 109595), ('inactive_split.all.peak', 58), ('inactive_split.large_pool.allocated', 8676), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 8674), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 100924), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 100921), ('inactive_split.small_pool.peak', 54), ('inactive_split_bytes.all.allocated', 118656369152), ('inactive_split_bytes.all.current', 13271040), ('inactive_split_bytes.all.freed', 118643098112), ('inactive_split_bytes.all.peak', 34571264), ('inactive_split_bytes.large_pool.allocated', 46435643392), ('inactive_split_bytes.large_pool.current', 11654656), ('inactive_split_bytes.large_pool.freed', 46423988736), ('inactive_split_bytes.large_pool.peak', 29268992), ('inactive_split_bytes.small_pool.allocated', 72220725760), ('inactive_split_bytes.small_pool.current', 1616384), ('inactive_split_bytes.small_pool.freed', 72219109376), ('inactive_split_bytes.small_pool.peak', 9044480), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('reserved_bytes.all.allocated', 1155530752), ('reserved_bytes.all.current', 1155530752), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 1155530752), ('reserved_bytes.large_pool.allocated', 1063256064), ('reserved_bytes.large_pool.current', 1063256064), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 1063256064), ('reserved_bytes.small_pool.allocated', 92274688), ('reserved_bytes.small_pool.current', 92274688), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 92274688), ('segment.all.allocated', 48), ('segment.all.current', 48), ('segment.all.freed', 0), ('segment.all.peak', 48), ('segment.large_pool.allocated', 4), ('segment.large_pool.current', 4), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 4), ('segment.small_pool.allocated', 44), ('segment.small_pool.current', 44), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 44)])\n",
            " Run   1, iteration:   5:   Loss at step 1: 0.1915283203125, mean for epoch: 0.1915283203125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 2: 0.185302734375, mean for epoch: 0.18841552734375, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 3: 0.1990966796875, mean for epoch: 0.19197591145833334, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 4: 0.16845703125, mean for epoch: 0.18609619140625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 5: 0.2117919921875, mean for epoch: 0.1912353515625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 6: 0.2091064453125, mean for epoch: 0.1942138671875, mem_alloc: 1140746752\n",
            "[2022-05-08 11:23:00,701] [INFO] [logging.py:69:log_dist] [Rank 0] step=170, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:23:00,702] [INFO] [timer.py:201:stop] 0/170, SamplesPerSec=8.53463784820393, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   5:   Loss at step 7: 0.243896484375, mean for epoch: 0.20131138392857142, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 8: 0.1533203125, mean for epoch: 0.1953125, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 9: 0.1729736328125, mean for epoch: 0.19283040364583334, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 10: 0.14892578125, mean for epoch: 0.18843994140625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 11: 0.147705078125, mean for epoch: 0.18473677201704544, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 12: 0.1414794921875, mean for epoch: 0.18113199869791666, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 13: 0.1318359375, mean for epoch: 0.1773399939903846, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 14: 0.1533203125, mean for epoch: 0.17562430245535715, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 15: 0.1378173828125, mean for epoch: 0.17310384114583333, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 16: 0.125732421875, mean for epoch: 0.17014312744140625, mem_alloc: 1140746752\n",
            "[2022-05-08 11:23:06,624] [INFO] [logging.py:69:log_dist] [Rank 0] step=180, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:23:06,625] [INFO] [timer.py:201:stop] 0/180, SamplesPerSec=8.532553227533723, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   5:   Loss at step 17: 0.1400146484375, mean for epoch: 0.16837086397058823, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 18: 0.130615234375, mean for epoch: 0.16627332899305555, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 19: 0.2120361328125, mean for epoch: 0.16868189761513158, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 20: 0.164306640625, mean for epoch: 0.168463134765625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 21: 0.1590576171875, mean for epoch: 0.16801525297619047, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 22: 0.1436767578125, mean for epoch: 0.16690895774147727, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 23: 0.10791015625, mean for epoch: 0.16434379245923914, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 24: 0.202880859375, mean for epoch: 0.16594950358072916, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 25: 0.18310546875, mean for epoch: 0.1666357421875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 26: 0.1727294921875, mean for epoch: 0.1668701171875, mem_alloc: 1140746752\n",
            "[2022-05-08 11:23:12,535] [INFO] [logging.py:69:log_dist] [Rank 0] step=190, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:23:12,536] [INFO] [timer.py:201:stop] 0/190, SamplesPerSec=8.531776898151339, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   5:   Loss at step 27: 0.174560546875, mean for epoch: 0.16715494791666666, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 28: 0.211669921875, mean for epoch: 0.16874476841517858, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 29: 0.171142578125, mean for epoch: 0.1688274515086207, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 30: 0.1260986328125, mean for epoch: 0.16740315755208332, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 31: 0.103271484375, mean for epoch: 0.1653343939012097, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 32: 0.156494140625, mean for epoch: 0.16505813598632812, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 33: 0.2135009765625, mean for epoch: 0.16652610085227273, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 34: 0.1475830078125, mean for epoch: 0.16596895105698528, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 35: 0.177001953125, mean for epoch: 0.1662841796875, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 36: 0.1551513671875, mean for epoch: 0.16597493489583334, mem_alloc: 1140746752\n",
            "[2022-05-08 11:23:18,454] [INFO] [logging.py:69:log_dist] [Rank 0] step=200, skipped=18, lr=[5e-05], mom=[(0.9, 0.999)]\n",
            "[2022-05-08 11:23:18,454] [INFO] [timer.py:201:stop] 0/200, SamplesPerSec=8.530366264201843, MemAllocated=0.01GB, MaxMemAllocated=1.06GB\n",
            " Run   1, iteration:   5:   Loss at step 37: 0.1734619140625, mean for epoch: 0.16617728568412163, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 38: 0.1297607421875, mean for epoch: 0.16521895559210525, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 39: 0.152587890625, mean for epoch: 0.16489508213141027, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 40: 0.1644287109375, mean for epoch: 0.1648834228515625, mem_alloc: 1140746752\n",
            " Run   1, iteration:   5:   Loss at step 41: 0.14501953125, mean for epoch: 0.16439893769054878, mem_alloc: 1140746752\n",
            "test shape: (2624, 24, 1) (2624, 24, 1)\n",
            "Loss after iteration 5 ; MSE: 0.1644287109375, MAE: 0.21923828125\n",
            "Connected by ('127.0.0.1', 44472)\n",
            "\u001b[94mReceived training result: b'5;0.16443;0.21924' \u001b[0m\n",
            "Time per iteration 24.32281126976013, memory OrderedDict([('active.all.allocated', 353230), ('active.all.current', 7), ('active.all.freed', 353223), ('active.all.peak', 236), ('active.large_pool.allocated', 23937), ('active.large_pool.current', 5), ('active.large_pool.freed', 23932), ('active.large_pool.peak', 32), ('active.small_pool.allocated', 329293), ('active.small_pool.current', 2), ('active.small_pool.freed', 329291), ('active.small_pool.peak', 205), ('active_bytes.all.allocated', 330652080128), ('active_bytes.all.current', 9797632), ('active_bytes.all.freed', 330642282496), ('active_bytes.all.peak', 1140746752), ('active_bytes.large_pool.allocated', 250336214528), ('active_bytes.large_pool.current', 9316864), ('active_bytes.large_pool.freed', 250326897664), ('active_bytes.large_pool.peak', 1055092224), ('active_bytes.small_pool.allocated', 80315865600), ('active_bytes.small_pool.current', 480768), ('active_bytes.small_pool.freed', 80315384832), ('active_bytes.small_pool.peak', 87227392), ('allocated_bytes.all.allocated', 330652080128), ('allocated_bytes.all.current', 8632832), ('allocated_bytes.all.freed', 330643447296), ('allocated_bytes.all.peak', 1140746752), ('allocated_bytes.large_pool.allocated', 250336214528), ('allocated_bytes.large_pool.current', 8152064), ('allocated_bytes.large_pool.freed', 250328062464), ('allocated_bytes.large_pool.peak', 1055092224), ('allocated_bytes.small_pool.allocated', 80315865600), ('allocated_bytes.small_pool.current', 480768), ('allocated_bytes.small_pool.freed', 80315384832), ('allocated_bytes.small_pool.peak', 87227392), ('allocation.all.allocated', 353230), ('allocation.all.current', 6), ('allocation.all.freed', 353224), ('allocation.all.peak', 236), ('allocation.large_pool.allocated', 23937), ('allocation.large_pool.current', 4), ('allocation.large_pool.freed', 23933), ('allocation.large_pool.peak', 32), ('allocation.small_pool.allocated', 329293), ('allocation.small_pool.current', 2), ('allocation.small_pool.freed', 329291), ('allocation.small_pool.peak', 205), ('inactive_split.all.allocated', 137795), ('inactive_split.all.current', 5), ('inactive_split.all.freed', 137790), ('inactive_split.all.peak', 58), ('inactive_split.large_pool.allocated', 10849), ('inactive_split.large_pool.current', 2), ('inactive_split.large_pool.freed', 10847), ('inactive_split.large_pool.peak', 6), ('inactive_split.small_pool.allocated', 126946), ('inactive_split.small_pool.current', 3), ('inactive_split.small_pool.freed', 126943), ('inactive_split.small_pool.peak', 54), ('inactive_split_bytes.all.allocated', 148367173120), ('inactive_split_bytes.all.current', 13271040), ('inactive_split_bytes.all.freed', 148353902080), ('inactive_split_bytes.all.peak', 34571264), ('inactive_split_bytes.large_pool.allocated', 58059984384), ('inactive_split_bytes.large_pool.current', 11654656), ('inactive_split_bytes.large_pool.freed', 58048329728), ('inactive_split_bytes.large_pool.peak', 29268992), ('inactive_split_bytes.small_pool.allocated', 90307188736), ('inactive_split_bytes.small_pool.current', 1616384), ('inactive_split_bytes.small_pool.freed', 90305572352), ('inactive_split_bytes.small_pool.peak', 9044480), ('max_split_size', -1), ('num_alloc_retries', 0), ('num_ooms', 0), ('oversize_allocations.allocated', 0), ('oversize_allocations.current', 0), ('oversize_allocations.freed', 0), ('oversize_allocations.peak', 0), ('oversize_segments.allocated', 0), ('oversize_segments.current', 0), ('oversize_segments.freed', 0), ('oversize_segments.peak', 0), ('reserved_bytes.all.allocated', 1155530752), ('reserved_bytes.all.current', 1155530752), ('reserved_bytes.all.freed', 0), ('reserved_bytes.all.peak', 1155530752), ('reserved_bytes.large_pool.allocated', 1063256064), ('reserved_bytes.large_pool.current', 1063256064), ('reserved_bytes.large_pool.freed', 0), ('reserved_bytes.large_pool.peak', 1063256064), ('reserved_bytes.small_pool.allocated', 92274688), ('reserved_bytes.small_pool.current', 92274688), ('reserved_bytes.small_pool.freed', 0), ('reserved_bytes.small_pool.peak', 92274688), ('segment.all.allocated', 48), ('segment.all.current', 48), ('segment.all.freed', 0), ('segment.all.peak', 48), ('segment.large_pool.allocated', 4), ('segment.large_pool.current', 4), ('segment.large_pool.freed', 0), ('segment.large_pool.peak', 4), ('segment.small_pool.allocated', 44), ('segment.small_pool.current', 44), ('segment.small_pool.freed', 0), ('segment.small_pool.peak', 44)])\n",
            "1140746752\n",
            "test 768\n",
            "Validation set Loss at step 1: 0.01197052001953125, mean for epoch: 0.01197052001953125, mem_alloc: 1140746752\n",
            "Validation set Loss at step 2: 0.12408447265625, mean for epoch: 0.06802749633789062, mem_alloc: 1140746752\n",
            "Validation set Loss at step 3: 0.22314453125, mean for epoch: 0.11973317464192708, mem_alloc: 1140746752\n",
            "Validation set Loss at step 4: 0.0111846923828125, mean for epoch: 0.09259605407714844, mem_alloc: 1140746752\n",
            "Validation set Loss at step 5: 0.3369140625, mean for epoch: 0.14145965576171876, mem_alloc: 1140746752\n",
            "Validation set Loss at step 6: 0.01061248779296875, mean for epoch: 0.11965179443359375, mem_alloc: 1140746752\n",
            "Validation set Loss at step 7: 0.1431884765625, mean for epoch: 0.12301417759486608, mem_alloc: 1140746752\n",
            "Validation set Loss at step 8: 0.2005615234375, mean for epoch: 0.1327075958251953, mem_alloc: 1140746752\n",
            "Validation set Loss at step 9: 0.0113677978515625, mean for epoch: 0.11922539605034722, mem_alloc: 1140746752\n",
            "Validation set Loss at step 10: 0.33349609375, mean for epoch: 0.1406524658203125, mem_alloc: 1140746752\n",
            "Validation set Loss at step 11: 0.01029205322265625, mean for epoch: 0.12880151922052557, mem_alloc: 1140746752\n",
            "Validation set Loss at step 12: 0.165771484375, mean for epoch: 0.1318823496500651, mem_alloc: 1140746752\n",
            "Validation set Loss at step 13: 0.1806640625, mean for epoch: 0.1356347891000601, mem_alloc: 1140746752\n",
            "Validation set Loss at step 14: 0.01290130615234375, mean for epoch: 0.12686811174665177, mem_alloc: 1140746752\n",
            "Validation set Loss at step 15: 0.331298828125, mean for epoch: 0.140496826171875, mem_alloc: 1140746752\n",
            "Validation set Loss at step 16: 0.0100860595703125, mean for epoch: 0.13234615325927734, mem_alloc: 1140746752\n",
            "Validation set Loss at step 17: 0.191650390625, mean for epoch: 0.13583463781020222, mem_alloc: 1140746752\n",
            "Validation set Loss at step 18: 0.15087890625, mean for epoch: 0.1366704305013021, mem_alloc: 1140746752\n",
            "Validation set Loss at step 19: 0.019622802734375, mean for epoch: 0.13051002903988487, mem_alloc: 1140746752\n",
            "Validation set Loss at step 20: 0.326171875, mean for epoch: 0.14029312133789062, mem_alloc: 1140746752\n",
            "Validation set Loss at step 21: 0.01030731201171875, mean for epoch: 0.1341033208937872, mem_alloc: 1140746752\n",
            "Validation set Loss at step 22: 0.2236328125, mean for epoch: 0.13817284323952414, mem_alloc: 1140746752\n",
            "Validation set Loss at step 23: 0.11663818359375, mean for epoch: 0.13723655368970789, mem_alloc: 1140746752\n",
            "Validation set Loss at step 24: 0.040191650390625, mean for epoch: 0.1331930160522461, mem_alloc: 1140746752\n",
            "test shape: (768, 24, 1) (768, 24, 1)\n",
            "Loss for validation set  ; MSE: 0.1331787109375, MAE: 0.1890869140625\n",
            "Connected by ('127.0.0.1', 44488)\n",
            "\u001b[94mReceived result: b'0.13318;0.18909' \u001b[0m\n",
            "Iteration  1| MSE 0.1332 | MAE 0.1891\n",
            "Mean        | MSE 0.1332 | MAE 0.1891\n",
            "Iteration  1| MSE 0.1332 | MAE 0.1891\n",
            "Mean        | MSE 0.1332 | MAE 0.1891\n",
            "[[[1.00488, 0.75293]], [[0.39551, 0.43115]], [[0.2771, 0.33496]], [[0.20459, 0.26636]], [[0.16443, 0.21924]]]\n",
            "[2022-05-08 11:23:25,487] [INFO] [launch.py:210:main] Process 430 exits successfully.\n"
          ]
        }
      ]
    }
  ]
}